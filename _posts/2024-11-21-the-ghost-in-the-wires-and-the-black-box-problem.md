---
published: true
date: 2022-08-11
title: The Ghost in the Wires and the Black Box Problem
---
As a child, I saw the movie “I, Robot,” and it left me wondering what consciousness really was — could a machine really think, or act, or feel on its own? Could humans really ever create an artificial intelligence that could be truly conscious? And how was it that consciousness itself existed in humans?

Specifically there was a quote that really got me thinking.

“there has always been ghosts in the machine, random segments of code that have grouped together to form unexpected protocols. Unanticipated these free radicals engender questions of free will creativity and even the nature of… the soul…When does a perceptual schematic become consciousness? When does the difference engine become the search for truth? When does the personality simulation become the bitter mote of a soul?””

I interpreted the quote to mean that out of unintentional errors and interactions in the coding, consciousness had formed in the robots. From that, I decided that human consciousness must also arise from unintentional crosstalk in the brain, and never really thought about it again.

No longer a child, and armed with a degree in biology, I find myself coming back to this and can now articulate it in a way that makes sense to me.

— — — — — — — — — — — — — — — — — — — — — — — — — — — — —

In short, I believe that consciousness and complex thought arise from the unintended interactions between structures in the mind. Consciousness is an emergent property of biological chaos.

The brain is an unimaginably complex network of billions upon billions overlapping synapses and neurons forming structures governed by minuscule electric forces. Take any one pathway, just one circuit, and you’ll find it touching and interacting with thousands of others. Even a single hyper specialized neuron in a specific circuit can affect thousands of others.

In a biochemical system, unlike a computer, nothing is ever in a discrete state, fully on, or fully off. Even when something is “off” there are still small amounts of activity leaking though. A computer functions by flipping switches, to a 1 or a zero, on or off. The brain functions more like a dimmer switch — there are potentially an infinite amount of possible states, which heralds infinite complexity. Functionally the brain is a messy analog system, with so much going on at any given time, so many connections being made, that we cannot hope to know them all. And this where consciousness emerges from, out of messy, disordered chaos.

Personally, I think the nitty gritty details about how consciousness forms is a few magnitudes of order above our collective ability to understand. Our minds haven’t quite evolved the sophistication to really take stock of the billions of tiny pieces and meta-interactions that have resulted in the generation of consciousness.

Consciousness is basically a rube goldberg machine built of smaller rube goldberg machines, all the way down like nesting dolls. It’s inscrutable, a black box. So how do we get there for AI?

In the case of artificial intelligence, I don’t think any algorithm designed by a human will ever be truly “intelligent.” Man simply cannot create intelligence. Sorry. It’s too complex and too chaotic.

_However_, a machine learning algorithm designed to create intelligence based off training data? That could be another story. How exactly algorithms arrive to conclusions is pretty opaque. When using advance statistical formulas and neural processing, we often don’t know how or why the model is the way it is. This the black box problem in AI. But for developing a generalist artificial intelligence, the black box might contain a gift.

If, given enough processing power and the right hardware (let’s say polymorphic processors), could the right algorithm create general intelligence? Assuming yes, and considering the complex underpinnings of consciousness are too complicated for humans, and the reasoning of black box AIs is inscrutable to us (Not to say that not being able to understand the black box means it _can_ create consciousness, necessarily), could we even know if consciousness has been created at all?

This brings us to solipsism: can we be sure that anyone other than ourselves is actually conscious? Consider the philosophical zombie — other seemingly conscious beings that actually lack true consciousness and instead only display traits of consciousness to the observer. Every other person out there could simply be simulations of consciousness. If a black box algorithm created a black box general intelligence, how would we even know? We’re not even fully aware of our own consciousness. How is it defined? The capacity to experience emotions, awareness of the external environment? Awareness of the self? The ability to remember experiences and learn from them? An internal monologue (Which some people [don’t have](https://www.livescience.com/does-everyone-have-inner-monologue.html)?) A sense of selfhood?

All that seems like it could be sufficiently simulated. Let’s say we have a nice program that covers all the bases. We layer a couple algorithms (all made for us by our friendly black box algorithm) — one each for emotions, reasoning, memory, learning, a couple rudimentary “senses”, a “body” and some enviromental and internal parameters that result in “death.” And then on top of that, another algorithm that “coordinates” all the other ones so they all affect each other. Then we stick it in a simulated changing environment, with other programs that approximate its own behavior (but are basically “dumb” NPCs), that it must learn to navigate and survive in, and to do that it must learn and rely on its algorithms for sense, reasoning, memory, and emotion.

Not exactly sentient, but we’ve basically roughly simulated what I think would be mouse level intelligence. Sure, it’s constrained to its programmed world, has limited reasoning capability, and its behavior is programmed, but does that make it “not alive?” Could it ever know that it was a simulation, and that the other programs, despite superficially resembling it, were also simulated and possessed no volition of their own?

Keep in mind, human behavior is coded by our DNA (ignoring culture for the sake of simplicity). We’re effectively constrained to our world and by physical laws that are mathematically governed, our capacity for reasoning isn’t infinite and our interactions with our enviroment are mediated by just a couple of senses. It seems like we have quite a lot in common with our hypothetical algorithm mouse. Could you yourself be a highly complex program running in a simulation? [Probably.](https://www.robotecture.com/tech/the-simulation-theory-how-to-tell-if-we-are-living-in-a-computer-simulation/)

Coming back to the black box problem and the solipsism in general, we know our algorithms are simulating emotions, memory, etc, but not how. And we also don’t know how they’re interacting with each other and the enviroment. We don’t know 100% how exactly that program is working, what it’s feeling, how it’s interpreting things. Maybe it has an internal monologue, a sense of self, maybe even self directed goals. Who knows. It could have an inner consciousness as rich as you.

Let’s pull the algorithm out of the ether of the digital world, make it a little more advanced, and give it a physical form, with all the attributes it had but now with a physical body, senses comparable to our own and a shared environment with us. It’s not super intelligent, it can’t do complex mathematical equations in its head because “it’s a computer,” it doesn’t connect to the internet via telepathic wifi. It needs to learn things like we do — the algorithms don’t contain innate knowledge of the world, they simply learn. It would need to learn to walk, and keep it’s balance, and communicate, to read. Everything a child has to learn, an artificial intelligence would have to learn. Its consciousness is abstracted from all the ones and zeroes and all the complexity that went into making it think, just like we’re abstracted from the immense complexity that goes into making us think. It doesn’t matter that at the base level it is silicon neurons instead of biological ones firing, that is not where consciousness emerges from. It emerges from the chaos and the crosstalk between structures, and the black box of our algorithms provides those structures and mediates the crosstalk for our AI.

I think, considering all this, a “general” human level artificial intelligence could be achieved, though it would be quite different than the popular boogeyman AI that seem to capture all the attention these days. Alas, I am just a peasant biologist with no formal training in machine learning and this is a pet theory, but maybe it will resonate with others.